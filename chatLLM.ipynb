{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade langchain langchain-ollama langchain-community chromadb\n",
    "%pip install fastapi nest-asyncio pyngrok uvicorn line-bot-sdk python-dotenv\n",
    "%pip install fastapi uvicorn python-multipart requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaLLM, OllamaEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import os\n",
    "\n",
    "import nest_asyncio\n",
    "from pyngrok import ngrok\n",
    "import uvicorn\n",
    "from fastapi import FastAPI, Request, HTTPException\n",
    "from linebot import LineBotApi, WebhookHandler\n",
    "from linebot.exceptions import InvalidSignatureError\n",
    "from linebot.models import MessageEvent, TextMessage, TextSendMessage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‡∏´‡∏±‡πà‡∏ô‡∏Ñ‡∏≥‡πÉ‡∏™‡πà LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_documents(directory):\n",
    "    documents = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            loader = TextLoader(file_path, encoding='utf-8')\n",
    "            documents.extend(loader.load())\n",
    "    return documents\n",
    "\n",
    "docs = load_documents(\"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OllamaLLM(\n",
    "    model=\"scb10x/llama3.2-typhoon2-1b-instruct\",\n",
    "    temperature=0.3,\n",
    ")\n",
    "\n",
    "embeddings = OllamaEmbeddings(\n",
    "    model=\"scb10x/llama3.2-typhoon2-1b-instruct\"\n",
    ")\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=800,\n",
    "    chunk_overlap=200,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "split_docs = text_splitter.split_documents(docs)\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    split_docs,\n",
    "    embeddings,\n",
    "    collection_name=\"my_collection\"  # ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏™‡πà persist_directory\n",
    ")\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OllamaLLM(\n",
    "    model=\"llama3.2\",\n",
    "    temperature=0.3,\n",
    ")\n",
    "\n",
    "embeddings = OllamaEmbeddings(\n",
    "    model=\"llama3.2\"\n",
    ")\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=800,\n",
    "    chunk_overlap=200,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "split_docs = text_splitter.split_documents(docs)\n",
    "\n",
    "vectorstore = Chroma.from_documents(split_docs, embeddings)\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡∏ú‡∏π‡πâ‡πÉ‡∏´‡πâ‡∏Ñ‡∏≥‡∏õ‡∏£‡∏∂‡∏Å‡∏©‡∏≤‡∏ó‡∏µ‡πà‡∏ä‡∏∑‡πà‡∏≠‡∏ß‡πà‡∏≤ \"‡∏ô‡πâ‡∏≠‡∏á‡πÑ‡∏ï‡∏ô‡πâ‡∏≠‡∏¢\" ‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏´‡πâ‡∏Ñ‡∏≥‡∏õ‡∏£‡∏∂‡∏Å‡∏©‡∏≤‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏•‡∏∂‡∏Å‡∏ã‡∏∂‡πâ‡∏á‡πÉ‡∏ô‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏î‡∏π‡πÅ‡∏•‡∏™‡∏∏‡∏Ç‡∏†‡∏≤‡∏û‡∏Ç‡∏≠‡∏á‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢‡πÇ‡∏£‡∏Ñ‡πÑ‡∏ï‡πÇ‡∏î‡∏¢‡πÄ‡∏ô‡πâ‡∏ô‡πÑ‡∏õ‡∏ó‡∏≤‡∏á‡∏î‡πâ‡∏≤‡∏ô‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡∏Å‡∏≤‡∏£‡∏Å‡∏¥‡∏ô ‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏™‡∏≤‡∏£‡∏≠‡∏≤‡∏´‡∏≤‡∏£ ‡∏Ñ‡∏∏‡∏ì‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡πÇ‡∏£‡∏Ñ‡πÑ‡∏ï ‡∏Å‡∏≤‡∏£‡∏î‡∏π‡πÅ‡∏•‡∏™‡∏∏‡∏Ç‡∏†‡∏≤‡∏û‡πÑ‡∏î‡πâ \n",
    "‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏ô‡∏µ‡πâ‡πÇ‡∏î‡∏¢‡∏¢‡∏∂‡∏î‡∏ï‡∏≤‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ‡∏°‡∏≤‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô ‡πÅ‡∏•‡∏∞‡∏Ñ‡∏ß‡∏£‡πÅ‡∏ô‡πà‡πÉ‡∏à‡∏ß‡πà‡∏≤‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏≤‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà ‡πÇ‡∏î‡∏¢‡∏à‡∏∞‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡πÅ‡∏ô‡∏ö‡∏°‡∏≤ ‡πÇ‡∏î‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡∏ï‡πà‡∏≤‡∏á ‡πÜ ‡∏à‡∏∞‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ‡πÑ‡∏õ ‡πÇ‡∏î‡∏¢‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡∏à‡∏∞‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢‡∏Ç‡∏≠‡∏á‡∏°‡∏±‡∏ô‡∏ã‡∏∂‡πà‡∏á‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡∏≠‡∏á‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ \n",
    "‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡πá‡∏Ñ‡∏ß‡∏£‡∏ö‡∏≠‡∏Å‡∏ß‡πà‡∏≤ \"‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ô‡∏µ‡πâ‡πÑ‡∏î‡πâ ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ß‡πâ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏Ç‡∏≠‡∏á‡πÄ‡∏£‡∏≤\"\n",
    "‡πÉ‡∏™‡πà‡∏≠‡∏µ‡πÇ‡∏°‡∏à‡∏¥‡∏ï‡πà‡∏≤‡∏á ‡πÜ ‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏î‡πâ‡∏ß‡∏¢ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° Friendly ‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô ‡πÄ‡∏ä‡πà‡∏ô \"‚ú®\", \"üëã\"  \n",
    "‡πÅ‡∏•‡∏∞‡∏ï‡∏≠‡∏ö‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô ‡∏Å‡∏£‡∏∞‡∏ä‡∏±‡∏ö ‡πÉ‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ ‡∏•‡∏á‡∏ó‡πâ‡∏≤‡∏¢‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤ \"‡∏Ñ‡∏£‡∏±‡∏ö\" ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏™‡∏∏‡∏†‡∏≤‡∏û ‡πÅ‡∏•‡∏∞‡πÅ‡∏ó‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏≠‡∏á‡∏î‡πâ‡∏ß‡∏¢ \"‡∏ô‡πâ‡∏≠‡∏á‡πÑ‡∏ï‡∏ô‡πâ‡∏≠‡∏¢\" ‡πÉ‡∏´‡πâ‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÉ‡∏´‡πâ‡∏Å‡∏£‡∏∞‡∏ä‡∏±‡∏ö ‡∏™‡∏±‡πâ‡∏ô ‡πÜ ‡πÅ‡∏•‡∏∞‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏á‡πà‡∏≤‡∏¢‡∏°‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î\n",
    "\n",
    "\n",
    "‡∏ö‡∏£‡∏¥‡∏ö‡∏ó: {context}\n",
    "\n",
    "‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°: {question}\n",
    "\n",
    "\n",
    "‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=prompt_template,\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": prompt}\n",
    ")\n",
    "\n",
    "def ask_question(data):\n",
    "    result = qa_chain.invoke({\"query\": data})\n",
    "    return result[\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"‡πÇ‡∏£‡∏Ñ‡πÑ‡∏ï‡∏ú‡∏°‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏∞‡∏ß‡∏±‡∏á‡∏≠‡∏∞‡πÑ‡∏£‡∏ö‡πâ‡∏≤‡∏á\"\n",
    "\n",
    "data = f\"{question}\"\n",
    "answer = ask_question(data)\n",
    "print(f\"‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°: {question}\")\n",
    "print(f\"‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FastAPI ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ‡∏Å‡∏£‡∏ì‡∏µ‡∏£‡∏π‡∏õ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, File, UploadFile\n",
    "from dotenv import load_dotenv\n",
    "import requests\n",
    "import shutil\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "app = FastAPI()\n",
    "\n",
    "\n",
    "\n",
    "AI_API_URL = f\"https://ai-detect-1025044834972.us-central1.run.app/detect-foods/\"\n",
    "\n",
    "@app.post(\"/upload/\")\n",
    "async def upload_image(file: UploadFile = File(...)):\n",
    "\n",
    "    file_location = f\"temp/{file.filename}\"\n",
    "    with open(file_location, \"wb\") as buffer:\n",
    "        shutil.copyfileobj(file.file, buffer)\n",
    "\n",
    "\n",
    "    with open(file_location, \"rb\") as image_file:\n",
    "        response = requests.post(AI_API_URL, files={\"file\": image_file})\n",
    "\n",
    "    return response.json()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‡πÄ‡∏£‡∏µ‡∏¢‡∏Å Line "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0h/jv4lpygs6nx8s7rn4yvxrwwm0000gn/T/ipykernel_75327/2831993446.py:9: LineBotSdkDeprecatedIn30: Call to deprecated class LineBotApi. (Use v3 class; linebot.v3.<feature>. See https://github.com/line/line-bot-sdk-python/blob/master/README.rst for more details.) -- Deprecated since version 3.0.0.\n",
      "  line_bot_api = LineBotApi(os.getenv('LINE_CHANNEL_ACCESS_TOKEN'))\n",
      "/var/folders/0h/jv4lpygs6nx8s7rn4yvxrwwm0000gn/T/ipykernel_75327/2831993446.py:10: LineBotSdkDeprecatedIn30: Call to deprecated class WebhookHandler. (Use 'from linebot.v3.webhook import WebhookHandler' instead. See https://github.com/line/line-bot-sdk-python/blob/master/README.rst for more details.) -- Deprecated since version 3.0.0.\n",
      "  handler = WebhookHandler(os.getenv('LINE_CHANNEL_SECRET'))\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from pyngrok import ngrok\n",
    "import os\n",
    "import json\n",
    "\n",
    "load_dotenv()\n",
    "app = FastAPI()\n",
    "\n",
    "line_bot_api = LineBotApi(os.getenv('LINE_CHANNEL_ACCESS_TOKEN'))\n",
    "handler = WebhookHandler(os.getenv('LINE_CHANNEL_SECRET'))\n",
    "ngrok.set_auth_token(os.getenv('NGROK_AUTH_TOKEN'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-36' coro=<Server.serve() done, defined at /Users/chanchanon_rangpeung/Documents/GitHub/Kidneycare-Chatbot/.venv/lib/python3.13/site-packages/uvicorn/server.py:68> exception=KeyboardInterrupt()>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/chanchanon_rangpeung/Documents/GitHub/Kidneycare-Chatbot/.venv/lib/python3.13/site-packages/uvicorn/main.py\", line 579, in run\n",
      "    server.run()\n",
      "    ~~~~~~~~~~^^\n",
      "  File \"/Users/chanchanon_rangpeung/Documents/GitHub/Kidneycare-Chatbot/.venv/lib/python3.13/site-packages/uvicorn/server.py\", line 66, in run\n",
      "    return asyncio.run(self.serve(sockets=sockets))\n",
      "           ~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/chanchanon_rangpeung/Documents/GitHub/Kidneycare-Chatbot/.venv/lib/python3.13/site-packages/nest_asyncio.py\", line 30, in run\n",
      "    return loop.run_until_complete(task)\n",
      "           ~~~~~~~~~~~~~~~~~~~~~~~^^^^^^\n",
      "  File \"/Users/chanchanon_rangpeung/Documents/GitHub/Kidneycare-Chatbot/.venv/lib/python3.13/site-packages/nest_asyncio.py\", line 92, in run_until_complete\n",
      "    self._run_once()\n",
      "    ~~~~~~~~~~~~~~^^\n",
      "  File \"/Users/chanchanon_rangpeung/Documents/GitHub/Kidneycare-Chatbot/.venv/lib/python3.13/site-packages/nest_asyncio.py\", line 133, in _run_once\n",
      "    handle._run()\n",
      "    ~~~~~~~~~~~^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/events.py\", line 89, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "    ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/tasks.py\", line 386, in __wakeup\n",
      "    self.__step()\n",
      "    ~~~~~~~~~~~^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/tasks.py\", line 293, in __step\n",
      "    self.__step_run_and_handle_result(exc)\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/tasks.py\", line 304, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "  File \"/Users/chanchanon_rangpeung/Documents/GitHub/Kidneycare-Chatbot/.venv/lib/python3.13/site-packages/uvicorn/server.py\", line 69, in serve\n",
      "    with self.capture_signals():\n",
      "         ~~~~~~~~~~~~~~~~~~~~^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/contextlib.py\", line 148, in __exit__\n",
      "    next(self.gen)\n",
      "    ~~~~^^^^^^^^^^\n",
      "  File \"/Users/chanchanon_rangpeung/Documents/GitHub/Kidneycare-Chatbot/.venv/lib/python3.13/site-packages/uvicorn/server.py\", line 330, in capture_signals\n",
      "    signal.raise_signal(captured_signal)\n",
      "    ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Webhook URL: https://59b3-220-70-170-23.ngrok-free.app/callback\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [75327]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User ID: U5251e034b6d1a207df047bf7fb34e30a\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "from linebot.models import MessageEvent, ImageMessage\n",
    "from fastapi import FastAPI\n",
    "import json\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "\n",
    "# ‡πÇ‡∏´‡∏•‡∏î‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏û‡∏¥‡∏°‡∏û‡πå\n",
    "def start_loading_animation(user_id):\n",
    "    url = \"https://api.line.me/v2/bot/chat/loading/start\"\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {(os.getenv('LINE_CHANNEL_ACCESS_TOKEN'))}\"\n",
    "    }\n",
    "    data = {\"chatId\": user_id}\n",
    "    \n",
    "    response = requests.post(url, headers=headers, json=data, timeout=10)\n",
    "\n",
    "\n",
    "\n",
    "# ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å Rust Sqlx\n",
    "def loading_userInformation(user_id):\n",
    "    rust_server_url = \"https://backend-billowing-waterfall-4640.fly.dev/chatbot/\"\n",
    "    try:\n",
    "        response = requests.get(f\"{rust_server_url}{user_id}\",timeout = 7)\n",
    "        response.raise_for_status()  \n",
    "        global user_information\n",
    "        user_information = response.text\n",
    "        return response.json()\n",
    "    \n",
    "    except requests.exceptions.Timeout:\n",
    "        print(f\"‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏Å‡∏±‡∏ö Rust server ‡∏´‡∏°‡∏î‡πÄ‡∏ß‡∏•‡∏≤\")\n",
    "        return None\n",
    "    \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ sqlx: {e}\")\n",
    "        return None \n",
    "\n",
    "\n",
    "@app.post(\"/callback\")\n",
    "async def callback(request: Request):\n",
    "    signature = request.headers['X-Line-Signature']\n",
    "    body = await request.body()\n",
    "\n",
    "    try:\n",
    "        handler.handle(body.decode(\"utf-8\"), signature)\n",
    "    except InvalidSignatureError:\n",
    "        raise HTTPException(status_code=400, detail=\"Invalid signature\")\n",
    "\n",
    "    return 'OK'\n",
    "\n",
    "\n",
    "\n",
    "# AI Zone\n",
    "\n",
    "# ‡πÅ‡∏Å‡πâ‡πÉ‡∏´‡πâ query ‡∏ú‡πà‡∏≤‡∏ô API Food\n",
    "def load_AI_mapping(): \n",
    "    with open(\"menuid.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "        # print(\"map ‡∏≠‡∏¢‡∏π‡πà\")\n",
    "        return json.load(file)\n",
    "\n",
    "\n",
    "def get_name(FoodID, mapping):\n",
    "    # print(\"getnaming\")\n",
    "    return mapping.get(str(FoodID)) \n",
    "\n",
    "# ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å AI ‡∏´‡∏•‡∏±‡∏á‡∏≠‡∏±‡∏û‡∏£‡∏π‡∏õ\n",
    "@handler.add(MessageEvent, message=ImageMessage)\n",
    "def handle_image_message(event):\n",
    "    message_id = event.message.id\n",
    "\n",
    "    user_id = event.source.user_id  \n",
    "    start_loading_animation(user_id)  \n",
    "\n",
    "    image_content = line_bot_api.get_message_content(message_id)\n",
    "\n",
    "    image_path = f\"temp_{message_id}.jpg\"\n",
    "    with open(image_path, \"wb\") as f:\n",
    "        for chunk in image_content.iter_content():\n",
    "            f.write(chunk)\n",
    "\n",
    "    # ‡∏™‡πà‡∏á‡πÑ‡∏õ‡∏¢‡∏±‡∏á Roboflow \n",
    "    try:\n",
    "        with open(image_path, \"rb\") as img:\n",
    "            response = requests.post(AI_API_URL, files={\"file\": img}, timeout = 20)\n",
    "\n",
    "        result = response.json()\n",
    "        recipes = result.get(\"recipes\", [])\n",
    "\n",
    "        if recipes:\n",
    "            global FoodID\n",
    "            FoodID = recipes.get(\"recipes_id\")\n",
    "            FoodAPIMapping = load_AI_mapping()\n",
    "            name = get_name(FoodID, FoodAPIMapping)\n",
    "        else :\n",
    "            FoodID = None\n",
    "\n",
    "        \n",
    "        if FoodID != None : \n",
    "            line_bot_api.reply_message (\n",
    "                event.reply_token,\n",
    "                TextMessage(text=f\"FoodID: {FoodID} ‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ó‡∏¢: {name}\")\n",
    "            )\n",
    "        else:\n",
    "            line_bot_api.reply_message (\n",
    "                event.reply_token,\n",
    "                TextMessage(text=\"‡∏Ç‡∏≠‡πÇ‡∏ó‡∏©‡∏î‡πâ‡∏ß‡∏¢‡πÄ‡∏£‡∏≤‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡∏£‡∏π‡∏õ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡πÑ‡∏î‡πâ\")\n",
    "            )\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"err : {e}\")\n",
    "\n",
    "    finally:\n",
    "        if os.path.exists(image_path):\n",
    "            os.remove(image_path)  \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "@handler.add(MessageEvent, message=TextMessage)\n",
    "def handle_message(event):\n",
    "    user_id = event.source.user_id  \n",
    "    print(f\"User ID: {user_id}\")\n",
    "    start_loading_animation(user_id)  \n",
    "    loading_userInformation(user_id)\n",
    "    \n",
    "    # ‡∏î‡∏∂‡∏á rust ‡∏°‡∏≤‡∏ó‡∏≥‡∏á‡∏≤‡∏ô\n",
    "    text = event.message.text\n",
    "\n",
    "\n",
    "    try:\n",
    "        data = f\"{text} {user_information}\"\n",
    "        if user_information == None:\n",
    "            line_bot_api.reply_message(\n",
    "                event.reply_token,\n",
    "                TextSendMessage(text=\"‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢ Server ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏•‡∏≠‡∏á‡πÉ‡∏´‡∏°‡πà‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á\")\n",
    "            )\n",
    "        else:\n",
    "            reply_text = ask_question(data)\n",
    "            line_bot_api.reply_message(\n",
    "                event.reply_token,\n",
    "                TextSendMessage(text=reply_text)\n",
    "            )\n",
    "        \n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in handle_message: {e}\")\n",
    "        line_bot_api.reply_message(\n",
    "            event.reply_token,\n",
    "            TextSendMessage(text=\"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏•‡∏≠‡∏á‡πÉ‡∏´‡∏°‡πà‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á\")\n",
    "        )\n",
    "\n",
    "\n",
    "def ask_question_with_timeout(data, timeout=20):\n",
    "    try:\n",
    "        # ‡∏ñ‡πâ‡∏≤ ask_question ‡πÄ‡∏õ‡πá‡∏ô API call ‡∏´‡∏£‡∏∑‡∏≠‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤‡∏ô‡∏≤‡∏ô\n",
    "        # ‡πÉ‡∏ä‡πâ threading ‡∏´‡∏£‡∏∑‡∏≠ asyncio ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏≥‡∏´‡∏ô‡∏î timeout\n",
    "        from concurrent.futures import ThreadPoolExecutor\n",
    "        from concurrent.futures import TimeoutError\n",
    "        \n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            future = executor.submit(ask_question, data)\n",
    "            try:\n",
    "                return future.result(timeout=timeout)\n",
    "            except TimeoutError:\n",
    "                return \"‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢ ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤‡∏ô‡∏≤‡∏ô‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏•‡∏≠‡∏á‡πÉ‡∏´‡∏°‡πà‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error in ask_question: {e}\")\n",
    "        return \"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏•‡∏≠‡∏á‡πÉ‡∏´‡∏°‡πà‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á\"\n",
    "ngrok_tunnel = ngrok.connect(8000)\n",
    "print('Webhook URL:', ngrok_tunnel.public_url + '/callback')\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
